{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # helps with the math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt # to plot error during training\n",
    "import ReadData as ReadData\n",
    "import sys\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from datetime import datetime\n",
    "#from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import DecisionSnippetFeatures as DecisionSnippetFeatures\n",
    "\n",
    "dataPath = \"../data/\"\n",
    "forestsPath = \"../tmp/forests/\"\n",
    "snippetsPath = \"../tmp/snippets/\"\n",
    "resultsPath = \"../tmp/results/\"\n",
    "reportsPath = \"../tmp/reports/\"\n",
    "\n",
    "dataset = sys.argv[1]\n",
    "#patterns_count = int(sys.argv[2])\n",
    "#dataset = 'magic'\n",
    "\n",
    "forest_types = ['RF']\n",
    "forest_depths = [5]\n",
    "sigma_values = [0.3]\n",
    "forest_size = 25\n",
    "maxPatternSize = 4\n",
    "scoring_function = 'accuracy'\n",
    "depth = 3\n",
    "learners = {'NB': MultinomialNB,\n",
    "            'SVM': LinearSVC}\n",
    "#            'LR': LogisticRegression}\n",
    "\n",
    "learners_parameters = {'NB': {},\n",
    "                       'SVM': {'max_iter': 10000}}\n",
    "#                       'LR': {'max_iter': 5000}}\n",
    "\n",
    "run_fit_models = False\n",
    "run_mining = False\n",
    "run_training = True\n",
    "run_eval = False\n",
    "\n",
    "verbose = True\n",
    "data_size = int(sys.argv[2])\n",
    "epochs = int(sys.argv[3])\n",
    "\n",
    "X_train, Y_train = ReadData.readData(dataset, 'train', dataPath)\n",
    "X_test, Y_test = ReadData.readData(dataset, 'test', dataPath)\n",
    "X_train_org = X_train\n",
    "X_train = X_train[:data_size]\n",
    "Y_train = Y_train[:data_size]\n",
    "Y_test = Y_train[:data_size]\n",
    "splits_1 = []\n",
    "splits_2 = []\n",
    "splits_3 = []\n",
    "\n",
    "\n",
    "\n",
    "for graph_file in filter(lambda x: x.endswith('.json'), sorted(os.listdir(os.path.join(snippetsPath, dataset)))):\n",
    "        snippets_file = os.path.join(snippetsPath, dataset, graph_file)\n",
    "        with open(snippets_file, 'r') as f_decision_snippets:\n",
    "        \n",
    "            frequentpatterns = json.load(f_decision_snippets)\n",
    "            pattern = frequentpatterns[0][\"pattern\"]\n",
    "            feature = pattern[\"feature\"]\n",
    "            split = pattern[\"split\"]\n",
    "            \n",
    "patterns_count = len(frequentpatterns)\n",
    "\n",
    "\n",
    "splits = [[0 for x in range(patterns_count * np.power(2,depth))] for y in range(depth)]\n",
    "x_extended = [[[np.inf for x in range(patterns_count * np.power(2,depth))] for y in range(depth)] for z in range(len(X_train))]\n",
    "x_out = [[0 for x in range(patterns_count * np.power(2,depth))] for z in range(len(X_train))]\n",
    "\n",
    "def traverse(x, x_index, pattern, pattern_index, depth):\n",
    "    if (depth == 0):\n",
    "        \n",
    "        split = pattern[\"pattern\"][\"split\"]\n",
    "        feature = pattern[\"pattern\"][\"feature\"]\n",
    "        pattern = pattern[\"pattern\"]\n",
    "        x_extended[x_index][depth][pattern_index] = x[feature]\n",
    "        splits[depth][pattern_index] = split\n",
    "        #print(str(depth)+',' + str(pattern_index) + ', ' + str(split))\n",
    "    \n",
    "    if (\"rightChild\" in pattern):\n",
    "        print('yes-right')\n",
    "    if (\"leftChild\" in pattern):\n",
    "        print('yes-left')    \n",
    "    if (\"rightChild\" in pattern or \"leftChild\" in pattern):\n",
    "        \n",
    "        depth += 1\n",
    "        if (\"leftChild\" in pattern and \"rightChild\" not in pattern):\n",
    "            pattern = pattern[\"leftChild\"]\n",
    "            split = pattern[\"split\"]\n",
    "            feature = pattern[\"feature\"]\n",
    "            splits[depth][pattern_index * pow(2,depth)] = split\n",
    "            x_extended[x_index][depth][pattern_index * pow(2,depth)] = x[feature]\n",
    "            traverse(x, x_index,pattern, pattern_index, depth)\n",
    "            \n",
    "        elif (\"rightChild\" in pattern and \"leftChild\" not in pattern):\n",
    "            pattern = pattern[\"rightChild\"]\n",
    "            split = pattern[\"split\"]\n",
    "            feature = pattern[\"feature\"]\n",
    "            splits[depth][pattern_index * pow(2,depth) + 1] = split\n",
    "            x_extended[x_index][depth][pattern_index * pow(2,depth) + 1] = x[feature]\n",
    "            traverse(x, x_index,pattern, pattern_index, depth)\n",
    "           \n",
    "        \n",
    "        elif (\"rightChild\" in pattern and \"leftChild\" in pattern):\n",
    "            pattern_left = pattern[\"leftChild\"]\n",
    "            split_left = pattern_left[\"split\"]\n",
    "            feature = pattern_left[\"feature\"]\n",
    "            splits[depth][pattern_index * pow(2,depth)] = split_left\n",
    "            x_extended[x_index][depth][pattern_index * pow(2,depth)] = x[feature]\n",
    "            traverse(x, x_index,pattern_left, pattern_index, depth)\n",
    "            pattern_right = pattern[\"rightChild\"]\n",
    "            feature = pattern_right[\"feature\"]\n",
    "            split_right = pattern_right[\"split\"]\n",
    "            splits[depth][pattern_index * pow(2,depth) + 1] = split_right\n",
    "            x_extended[x_index][depth][pattern_index * pow(2,depth) + 1] = x[feature]\n",
    "            traverse(x, x_index,pattern_right, pattern_index, depth)            \n",
    "   \n",
    "for j,x in enumerate(X_train):    \n",
    "    for i,pattern in enumerate(frequentpatterns):\n",
    "        print(i)\n",
    "        print(pattern)\n",
    "        traverse(x,j,pattern,i,0)\n",
    "          \n",
    " \n",
    "splits_1 = splits[0][:patterns_count * np.power(2,depth -1)]\n",
    "splits_2 = splits[1][:patterns_count * np.power(2,depth -1)]\n",
    "splits_3 = splits[2][:patterns_count * np.power(2,depth -1)]\n",
    "\n",
    "x_extended = np.array(x_extended)\n",
    "x_extended_1 = x_extended[:,0,:patterns_count * np.power(2,depth -1)]\n",
    "x_extended_2 = x_extended[:,1,:patterns_count * np.power(2,depth -1)]\n",
    "x_extended_3 = x_extended[:,2,:patterns_count * np.power(2,depth -1)]\n",
    "x_out = np.array(x_out)\n",
    "\n",
    "splits_1 = np.array(splits_1).reshape(len(splits_1),1).astype(dtype = np.float64)\n",
    "splits_2 = np.array(splits_2).reshape(len(splits_2),1).astype(dtype = np.float64)\n",
    "splits_3 = np.array(splits_3).reshape(len(splits_3),1).astype(dtype = np.float64)\n",
    "\n",
    "m = splits_1.mean()\n",
    "std = splits_1.std()\n",
    "\n",
    "def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "extended_x_train = []\n",
    "extended_x_test = []\n",
    "for x in X_train:\n",
    "    for p in frequentpatterns:\n",
    "        extended_x_train.append(x[p[\"pattern\"][\"feature\"]])\n",
    "        \n",
    "for x in X_test:\n",
    "    for p in frequentpatterns:\n",
    "        extended_x_test.append(x[p[\"pattern\"][\"feature\"]])        \n",
    "\n",
    "feature_class = []\n",
    "counter_left = 0\n",
    "counter_left_y1 = 0\n",
    "counter_right = 0\n",
    "counter_right_y1 = 0\n",
    "'''\n",
    "for pattern in frequentpatterns:\n",
    "    for i,x in enumerate(X_train_org):\n",
    "        if (x[pattern[\"pattern\"][\"feature\"]] < pattern[\"pattern\"][\"split\"]):\n",
    "            counter_left += 1\n",
    "            if (Y_train[i] == 1):\n",
    "                counter_left_y1 += 1\n",
    "                \n",
    "        else:\n",
    "            counter_right += 1\n",
    "            if (Y_train[i] == 1):\n",
    "                counter_right_y1 += 1\n",
    "        \n",
    "    \n",
    "    if (counter_left != 0):\n",
    "        #feature_class.append(0.5)\n",
    "        feature_class.append(counter_left_y1/counter_left)\n",
    "    else:\n",
    "        feature_class.append(0)\n",
    "    if (counter_right != 0): \n",
    "        #feature_class.append(-0.5)\n",
    "        feature_class.append(-counter_right_y1/counter_right)\n",
    "    else:\n",
    "        feature_class.append(0)    \n",
    "            \n",
    "    #feature_class.append(0.5)\n",
    "    #feature_class.append(-0.5)\n",
    "    print(counter_left)\n",
    "    print(counter_left_y1)\n",
    "    print(counter_right)\n",
    "    print(counter_right_y1)\n",
    "    \n",
    "    counter_left = 0  \n",
    "    counter_left_y1 = 0        \n",
    "    counter_right = 0\n",
    "    counter_right_y1 = 0\n",
    "            \n",
    "    \n",
    "    #pattern = frequentpatterns[0][\"pattern\"]\n",
    "    feature = pattern[\"pattern\"][\"feature\"]\n",
    "    split = pattern[\"pattern\"][\"split\"]\n",
    "    #print(feature)\n",
    "    #print(split)\n",
    "    #splits.append(pattern[\"pattern\"][\"split\"])\n",
    "print(feature_class)    \n",
    "'''       \n",
    "weights_1 = torch.tensor(splits_1.reshape(1,patterns_count * np.power(2,depth - 1)),dtype = torch.float)\n",
    "weights_2 = torch.tensor(splits_2.reshape(1,patterns_count * np.power(2,depth - 1)),dtype = torch.float)\n",
    "weights_3 = torch.tensor(splits_3.reshape(1,patterns_count * np.power(2,depth - 1)),dtype = torch.float)\n",
    "\n",
    "x_extended_1 = x_extended_1.reshape(data_size,patterns_count * np.power(2,depth - 1))\n",
    "x_extended_2 = x_extended_2.reshape(data_size,patterns_count * np.power(2,depth - 1))\n",
    "x_extended_3 = x_extended_3.reshape(data_size,patterns_count * np.power(2,depth - 1))\n",
    "x_out = x_out.reshape(data_size,patterns_count * np.power(2,depth))\n",
    "\n",
    "Y_train = Y_train.reshape(len(Y_train),1)\n",
    "Y_test = Y_test.reshape(len(Y_test),1)\n",
    "\n",
    "feature_class = np.ones((1,patterns_count * np.power(2,depth)))\n",
    "feature_class = torch.tensor(feature_class.reshape(patterns_count * np.power(2,depth),1),dtype = torch.double)\n",
    "\n",
    "\n",
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Neural_Network, self).__init__()\n",
    "       \n",
    "        self.bias_1 = Variable(torch.tensor(weights_1),requires_grad = True)\n",
    "        self.bias_2 = Variable(torch.tensor(weights_2),requires_grad = True)\n",
    "        self.bias_3 = Variable(torch.tensor(weights_3),requires_grad = True)\n",
    "        self.W2 = Variable(torch.tensor((feature_class),dtype = torch.float64),requires_grad = True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X_1 =  x_extended_1.subtract(self.bias_1)\n",
    "        X_1 = self.sigmoid(X_1) \n",
    "        X_1 = torch.stack((X_1,1 - X_1), 2).reshape(len(X_1),patterns_count * np.power(2,depth))\n",
    "        X_1 = Variable(torch.tensor(X_1))\n",
    "        \n",
    "        X_2 =  x_extended_2.subtract(self.bias_2)\n",
    "        X_2 = self.sigmoid(X_2)\n",
    "        X_2 = torch.stack((X_2,1 - X_2), 2).reshape(len(X_1),patterns_count * np.power(2,depth))\n",
    "        X_2 = Variable(torch.tensor(X_2))\n",
    "        \n",
    "        X_3 =  x_extended_3.subtract(self.bias_3)\n",
    "        X_3 = self.sigmoid(X_3)\n",
    "        X_3 = torch.stack((X_3,1 - X_3), 2).reshape(len(X_1),patterns_count * np.power(2,depth))\n",
    "        X_3 = Variable(torch.tensor(X_3))\n",
    "        \n",
    "        global x_out\n",
    "        X_out = x_out\n",
    "        X_out = torch.tensor(X_out,dtype = torch.float64)\n",
    "      \n",
    "        for j,x in enumerate(X_out):\n",
    "            for i in range(patterns_count * np.power(2,depth)):\n",
    "                X_out[j][i] = X_1[j][(int)(i/8)] * X_2[j][(int)(i/4)] * X_3[j][(int)(i/2)]   \n",
    "     \n",
    "        x_out = Variable(torch.tensor(x_out))\n",
    "        X = torch.matmul(X_out, self.W2)\n",
    "        o = self.sigmoid(X) # final activation function\n",
    "        return o\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def saveWeights(self, model):\n",
    "        torch.save(model, \"NN\")\n",
    "        \n",
    "    def predict(self):\n",
    "        print (\"Predicted data based on trained weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
    "        print (\"Output: \\n\" + str(self.forward(xPredicted)))\n",
    "\n",
    "def train(X, y):\n",
    "        \n",
    "        optimizer = torch.optim.SGD([{'params':[NN.bias_1,NN.bias_2,NN.bias_3,NN.W2], 'lr':0.01}])\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(NN(X),y)\n",
    "        train_loss = loss.item()\n",
    "        NN.bias_1.retain_grad()\n",
    "        NN.bias_2.retain_grad()\n",
    "        NN.bias_3.retain_grad()\n",
    "        NN.W2.retain_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return train_loss\n",
    "        \n",
    "def evaluation(X, y):\n",
    "        \n",
    "        loss = criterion(NN(X),y)\n",
    "        valid_loss = loss.item()\n",
    "        return valid_loss\n",
    "\n",
    "inputs_train = x_extended    \n",
    "X_train = torch.tensor((inputs_train), dtype=torch.float) \n",
    "Y_train = torch.tensor((Y_train), dtype=torch.double)\n",
    "\n",
    "X_test = torch.tensor((inputs_train), dtype=torch.float) \n",
    "Y_test = torch.tensor((Y_test), dtype=torch.double) \n",
    "\n",
    "x_extended_1 = torch.tensor(x_extended_1)\n",
    "x_extended_2 = torch.tensor(x_extended_2)\n",
    "x_extended_3 = torch.tensor(x_extended_3)\n",
    "ex_test = torch.tensor(extended_x_test)\n",
    "\n",
    "NN = Neural_Network()\n",
    "loss_list = [] \n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCELoss().cuda()\n",
    "\n",
    "valid_loss = 0.0\n",
    "min_valid_loss = np.inf\n",
    "\n",
    "now = datetime.now()\n",
    "results_file = os.path.join(resultsPath, dataset, 'train_rf_splits_' + str(now) + '.csv')            \n",
    "    \n",
    "results_list = []    \n",
    "max_acc = 0.0\n",
    "patience = 4000\n",
    "count_patience = 0\n",
    "\n",
    "\n",
    "for i in range(epochs): \n",
    "    \n",
    "    train_loss = train(X_train, Y_train)\n",
    "    valid_loss = evaluation(X_test,Y_test)\n",
    "    Y_pred = (NN(X_test) > 0.5).float()\n",
    "    accuracy = (Y_test == Y_pred).sum().item()/len(X_test)\n",
    "    results_list.append(str(i+1) + ',' + str(train_loss)+','+str(valid_loss)+','+str(accuracy))\n",
    "    if (i % 100 == 0):\n",
    "        print('Epoch: ' + str(i))\n",
    "        print('train loss: ' + str(train_loss))\n",
    "        print('eval loss: ' + str(valid_loss))\n",
    "        print('Accuracy: ' + str(accuracy))\n",
    "    \n",
    "    if (max_acc < accuracy):\n",
    "        max_acc = accuracy\n",
    "        count_patience = 0\n",
    "        #print('increased')\n",
    "    else: \n",
    "        if (count_patience < patience):\n",
    "            count_patience += 1\n",
    "           \n",
    "        else:\n",
    "            break\n",
    "          \n",
    "NN.saveWeights(NN)\n",
    "\n",
    "with open(results_file, 'w') as fout:\n",
    "    fout.write('Epoch, Train Loss, Valid Loss, Accuracy \\n')\n",
    "    for e in results_list:\n",
    "        fout.write(e + '\\n')\n",
    "\n",
    "new_weights_1 = (NN.bias_1 * std) + m\n",
    "new_weights_2 = (NN.bias_2 * std) + m\n",
    "new_weights_3 = (NN.bias_3 * std) + m\n",
    "\n",
    "snippets_file = os.path.join(snippetsPath, dataset, 'trained_splits_'+ str(now) +'_'+ graph_file )            \n",
    "with open(snippets_file, 'w') as f:\n",
    "    f.write('[')\n",
    "\n",
    "    for i,pattern in enumerate(frequentpatterns):\n",
    "        pattern[\"pattern\"][\"split\"] = float(\"{:.2f}\".format(new_weights_1[0][i].item()))\n",
    "        f.write(str(pattern).replace('\\'','\\\"')+',\\n')\n",
    "        \n",
    "    f.write(']')  \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
