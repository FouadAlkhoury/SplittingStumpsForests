{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b8dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "HERE ARE THE ACCURACIES ON TEST DATA OF THE ORIGINAL RANDOM FORESTS\n",
      "(don't worry, test data is not used for training)\n",
      "\n",
      "Fitting RF_2\n",
      "Testing  RF_2\n",
      "Total time: 0.009930767999321688 ms\n",
      "Saving model\n",
      "Accuracy: 0.6165\n",
      "Saving model to PKL on disk\n",
      "*** Summary ***\n",
      "#Examples\t #Features\t Accuracy\t Avg.Tree Height\n",
      "0.6165\t2.3333333333333344\n",
      "\n",
      "Fitting Models Time: 0:00:00.106240\n"
     ]
    }
   ],
   "source": [
    "# This is the (cleaned up) code accompanying the publication\n",
    "#\n",
    "# Pascal Welke, Fouad Alkhoury, Christian Bauckhage, Stefan Wrobel: Decision Snippet Features.\n",
    "# International Conference on Pattern Recognition (ICPR) 2021.\n",
    "#\n",
    "# Code was written by Pascal Welke and Fouad Alkhoury and is based on\n",
    "# code written by Sebastian BuschjÃ¤ger (TU Dortmund) that is used for \n",
    "# json-serialization of random forest models.\n",
    "\n",
    "\n",
    "# %% imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import pickle\n",
    "\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import ReadData\n",
    "import cString2json as cString2json\n",
    "import json2graphNoLeafEdgesWithSplitValues as json2graphNoLeafEdgesWithSplitValues\n",
    "from fitModels import fitModels\n",
    "import DecisionSnippetFeatures as DecisionSnippetFeatures\n",
    "import pruning\n",
    "import Forest\n",
    "import datetime\n",
    "from util import writeToReport\n",
    "import numpy as np\n",
    "# %% Parameters.\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "dataPath = \"../data/\"\n",
    "forestsPath = \"../tmp/forests/\"\n",
    "snippetsPath = \"../tmp/snippets/\"\n",
    "resultsPath = \"../tmp/results/\"\n",
    "reportsPath = \"../tmp/reports/\"\n",
    "\n",
    "# current valid options are ['sensorless', 'satlog', 'mnist', 'magic', 'spambase', 'letter', 'bank', 'adult', 'drinking']\n",
    "dataSet = 'satlog'\n",
    "# dataSet = 'adult'\n",
    "# dataSet = 'drinking'\n",
    "\n",
    "# possible forest_types ['RF', 'DT', 'ET']\n",
    "forest_types = ['RF']\n",
    "forest_depths = [2]\n",
    "sigma_values = [0.0,0.1,0.2,0.3]\n",
    "#forest_depths = [5, 10, 15, 20]\n",
    "forest_size = 25\n",
    "\n",
    "maxPatternSize = 2\n",
    "minThreshold = 2\n",
    "maxThreshold = 25\n",
    "\n",
    "scoring_function = 'accuracy'\n",
    "\n",
    "# learners that are to be used on top of Decision Snippet Features\n",
    "learners = {'DSF_NB': MultinomialNB,\n",
    "            'DSF_SVM': LinearSVC, \n",
    "            'DSF_LR': LogisticRegression}\n",
    "\n",
    "# specify parameters that are given at initialization\n",
    "learners_parameters = {'DSF_NB': {},\n",
    "                       'DSF_SVM': {'max_iter': 10000},\n",
    "                       'DSF_LR': {'max_iter': 1000}}\n",
    "\n",
    "\n",
    "# for quick debugging, let the whole thing run once. Afterwards, you may deactivate individual steps\n",
    "# each step stores its output for the subsequent step(s) to process\n",
    "run_fit_models = True\n",
    "run_mining = False\n",
    "run_training = True\n",
    "run_eval = True\n",
    "\n",
    "verbose = True\n",
    "\n",
    "fitting_models_time = datetime.timedelta()\n",
    "pruning_time = datetime.timedelta()\n",
    "\n",
    "\n",
    "\n",
    "# %% load data\n",
    "\n",
    "X_train, Y_train = ReadData.readData(dataSet, 'train', dataPath)\n",
    "X_test, Y_test = ReadData.readData(dataSet, 'test', dataPath)\n",
    "#print(len(X_train))\n",
    "#print(len(X_test))\n",
    "X = X_train\n",
    "\n",
    "# %% create forest data, evaluate and report accuracy on test data\n",
    "start_fitting_models = datetime.datetime.now()\n",
    "\n",
    "if run_fit_models:\n",
    "    print('\\n\\nHERE ARE THE ACCURACIES ON TEST DATA OF THE ORIGINAL RANDOM FORESTS\\n(don\\'t worry, test data is not used for training)\\n')\n",
    "\n",
    "    fitModels(roundSplit=True, \n",
    "              XTrain=X_train, YTrain=Y_train, \n",
    "              XTest=X_test, YTest=Y_test, \n",
    "              createTest=False, model_dir=os.path.join(forestsPath, dataSet), types=forest_types, forest_depths = forest_depths)\n",
    "\n",
    "end_fitting_models = datetime.datetime.now()\n",
    "fitting_models_time = (end_fitting_models- start_fitting_models)\n",
    "\n",
    "print('Fitting Models Time: '+str(fitting_models_time))    \n",
    "# Pruning\n",
    "report_model_dir = reportsPath+'/'+dataSet \n",
    "report_file = report_model_dir + '/report.txt'\n",
    "if not os.path.exists(report_model_dir):\n",
    "    os.makedirs(report_model_dir)\n",
    "        \n",
    "writeToReport(report_file,'Fitting Models Time \\t ' + str(fitting_models_time) + '\\n')\n",
    "\n",
    "start_pruning_total = datetime.datetime.now()\n",
    "writeToReport(report_file,'Pruning Time \\t ')\n",
    "writeToReport(report_file,'RF \\t Sigma \\t Pruning Time')\n",
    "\n",
    "'''\n",
    "for depth in forest_depths:\n",
    "    \n",
    "    input_file = forestsPath+dataSet+'/RF_'+ str(depth) +'.json'\n",
    "    #feature_vectors_file = X\n",
    "    \n",
    "    \n",
    "    f = Forest.Forest()\n",
    "    f.fromJSON(input_file)\n",
    "    t = f.trees[0]\n",
    "    \n",
    "    feature_vectors = X\n",
    "\n",
    "    for sigma in sigma_values:\n",
    "        \n",
    "        start_pruning = datetime.datetime.now()\n",
    "        \n",
    "        pruning.Min_DBN(feature_vectors, f, sigma)\n",
    "        pruning.post_processing(f)\n",
    "        #print(f.pstr())\n",
    "    \n",
    "        #save the pruned forest in a json file\n",
    "        sigma_as_string = '_'.join(str(sigma).split('.'))\n",
    "        output_file = forestsPath + dataSet + '/RF_'+ str(depth) +'_pruned_with_sigma_' + sigma_as_string + '.json'\n",
    "    \n",
    "        with open(output_file, 'w') as outfile:\n",
    "            outfile.write(f.str())\n",
    "            \n",
    "        end_pruning = datetime.datetime.now()\n",
    "        pruning_time = (end_pruning - start_pruning)\n",
    "        print('Pruning Time for RF_'+str(depth)+' and sigma='+str(sigma)+' : '+str(pruning_time)) \n",
    "        writeToReport(report_file, str(depth)+ '\\t ' +str(sigma)+ '\\t \\t' + str(pruning_time))\n",
    "    \n",
    "end_pruning_total = datetime.datetime.now()\n",
    "pruning_total_time = (end_pruning_total - start_pruning_total)\n",
    "\n",
    "print('Total Pruning Time: '+str(pruning_total_time))  \n",
    "writeToReport(report_file, 'Total Pruning Time: '+str(pruning_total_time) + '\\n')\n",
    "'''\n",
    "# %% compute decision snippets\n",
    "\n",
    "\n",
    "start_mining_total = datetime.datetime.now()\n",
    "\n",
    "writeToReport(report_file,'Mining Time \\t ')\n",
    "\n",
    "if run_mining:\n",
    "    print('\\n\\nFEEL FREE TO IGNORE THIS OUTPUT\\n')\n",
    "\n",
    "    def pattern_frequency_filter(f_patterns, frequency, f_filtered_patterns):\n",
    "        pattern_count = 0\n",
    "        for line in f_patterns:\n",
    "            tokens = line.split('\\t')\n",
    "            if int(tokens[0]) >= frequency:\n",
    "                f_filtered_patterns.write(line)\n",
    "                pattern_count += 1\n",
    "        return pattern_count\n",
    "\n",
    "    start_json_to_graph = datetime.datetime.now()\n",
    "    # translate json to graph files\n",
    "    for json_file in filter(lambda x: x.endswith('.json'), os.listdir(os.path.join(forestsPath, dataSet))):\n",
    "        #print(json_file[:-4])\n",
    "        graph_file = json_file[:-4] + 'graph'\n",
    "        with open(os.path.join(forestsPath, dataSet, json_file), 'r') as f_in:\n",
    "            with open(os.path.join(forestsPath, dataSet, graph_file), 'w') as f_out:\n",
    "                json2graphNoLeafEdgesWithSplitValues.main(f_in, f_out)\n",
    "\n",
    "    end_json_to_graph = datetime.datetime.now()\n",
    "    json_to_graph_time = (end_json_to_graph - start_json_to_graph)\n",
    "    print('Translating Time :' + str(json_to_graph_time)) \n",
    "    writeToReport(report_file, 'Translating Time :' + str(json_to_graph_time))    \n",
    "\n",
    "    # run frequent pattern mining\n",
    "    if not os.path.exists(os.path.join(snippetsPath, dataSet)):\n",
    "        os.makedirs(os.path.join(snippetsPath, dataSet))\n",
    "\n",
    "    for graph_file in filter(lambda x: x.endswith('.graph'), os.listdir(os.path.join(forestsPath, dataSet))):\n",
    "        \n",
    "        # pattern mining for smallest minThreshold\n",
    "        #print(f\"mining {minThreshold}-frequent patterns for {graph_file}\")\n",
    "        \n",
    "        start_mining = datetime.datetime.now()\n",
    "        print(\"mining \"+ str(minThreshold) + \" frequent patterns for \" + str(graph_file))\n",
    "        pattern_file=os.path.join(snippetsPath, dataSet, graph_file[:-6] + f'_t{minThreshold}.patterns')\n",
    "        feature_file=os.path.join(snippetsPath, dataSet, graph_file[:-6] + f'_t{minThreshold}.features')\n",
    "        log_file=os.path.join(snippetsPath, dataSet, graph_file[:-6] + f'_t{minThreshold}.logs')\n",
    "\n",
    "        args = ['../lwgr', '-erootedTrees', '-mbfs', f'-t{minThreshold}', f'-p{maxPatternSize}', \n",
    "                f'-o{pattern_file}', os.path.join(forestsPath, dataSet, graph_file)]\n",
    "        with open(feature_file, 'w') as f_out:\n",
    "            with open(log_file, 'w') as f_err:        \n",
    "                subprocess.run(args, stdout=f_out, stderr=f_err)\n",
    "\n",
    "        # filtering of patterns for larger thresholds\n",
    "        print(f\"filtering more frequent patterns for {graph_file}\")\n",
    "        for threshold in range(maxThreshold, minThreshold, -1):\n",
    "            filtered_pattern_file=os.path.join(snippetsPath, dataSet, graph_file[:-6] + f'_t{threshold}.patterns')\n",
    "            pattern_count = -1\n",
    "            with open(pattern_file, 'r') as f_patterns:\n",
    "                with open(filtered_pattern_file, 'w') as f_filtered_patterns:\n",
    "                    pattern_count = pattern_frequency_filter(f_patterns, threshold, f_filtered_patterns)\n",
    "            \n",
    "            # if there are no frequent patterns for given threshold, remove the file.\n",
    "            if pattern_count == 0:\n",
    "                os.remove(filtered_pattern_file)\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f'threshold {threshold}: {pattern_count} frequent patterns')\n",
    "\n",
    "\n",
    "        # transform canonical string format to json\n",
    "        for threshold in range(maxThreshold, minThreshold-1, -1):\n",
    "            filtered_pattern_file = os.path.join(snippetsPath, dataSet, graph_file[:-6] + f'_t{threshold}.patterns')\n",
    "            filtered_json_file = os.path.join(snippetsPath, dataSet, graph_file[:-6] + f'_t{threshold}.json')\n",
    "            try:\n",
    "                with open(filtered_pattern_file, 'r') as f_filtered_patterns:\n",
    "                    with open(filtered_json_file, 'w') as f_filtered_json:\n",
    "                        json_data = cString2json.parseCStringFileUpToSizePatterns(f_filtered_patterns, patternSize=maxPatternSize)\n",
    "                        f_filtered_json.write(json_data)\n",
    "            except EnvironmentError:\n",
    "                # this might happen if a certain threshold resulted in no frequent patterns and is OK\n",
    "                pass\n",
    "            \n",
    "            \n",
    "        end_mining = datetime.datetime.now()\n",
    "        mining_time = (end_mining - start_mining)\n",
    "        print('Mining Time for '+ graph_file +' : '+str(mining_time)) \n",
    "        writeToReport(report_file, 'Mining Time for '+ graph_file +' : '+str(mining_time))    \n",
    "\n",
    "end_mining_total = datetime.datetime.now()\n",
    "mining_total_time = (end_mining_total - start_mining_total)\n",
    "writeToReport(report_file, 'Total Mining Time: '+str(mining_total_time) + '\\n')\n",
    "\n",
    "# %% Training of classifiers. For later selection of best candidate learners, run xval on train to estimate generalization\n",
    "\n",
    "writeToReport(report_file,'Training Time \\t ')\n",
    "\n",
    "\n",
    "\n",
    "f = open(snippetsPath+'/'+'satlog_'+'/'+'RF_5_t2'+'.json')\n",
    "frequentpatterns = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92a299f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "print(len(frequentpatterns))\n",
    "fts = frequentpatterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499a0835",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-21d5dde59eed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mmax_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "        class_labels_list = ''\n",
    "        class_labels_list_P = ''\n",
    "\n",
    "        one_counter=0\n",
    "        zero_counter=0\n",
    "        max_id = 0\n",
    "        max_class_id = 0\n",
    "        trees_count = len(fts[0])\n",
    "\n",
    "        for i in range(0,len(fts)):\n",
    "            if (Y_train[i] > max_class_id):\n",
    "                max_class_id +=1\n",
    "\n",
    "\n",
    "            for j in range(0,len(fts[i])):\n",
    "                if (fts[i][j] > max_id):\n",
    "                    max_id = fts[i][j]\n",
    "                  \n",
    "        id_count= max_id + 1            \n",
    "        class_count = max_class_id + 1  \n",
    "\n",
    "        for i in range(0,class_count):\n",
    "            class_labels_list += 'Y = '+str(i)+','\n",
    "            class_labels_list_P += 'P(Y) = '+str(i)+','\n",
    "    \n",
    "            \n",
    "          \n",
    "        \n",
    "\n",
    "        Matrix = np.zeros((trees_count,id_count,class_count))\n",
    "\n",
    "\n",
    "        for i in range(0,len(fts)):\n",
    "            for j in range(0,trees_count):\n",
    "\n",
    "                     Matrix[j][fts[i][j]][Y_train[i]] +=1\n",
    "\n",
    "                            \n",
    " \n",
    "\n",
    "\n",
    "        \n",
    "        Prefered_Class_Matrix = np.zeros((trees_count,id_count))\n",
    "        Prefered_Class_Matrix_P = np.zeros((trees_count,id_count,class_count))\n",
    "        f= open(resultsPath+'/'+dataset+'/Details_Files/'+'DSF_as_RF_'+str(rf_depth)+'_t'+str(frequency)+'_'+dataset+'_n.csv',\"w\")\n",
    "        f.write('Feature ,Leaf id,'+class_labels_list+' Preferred Class, '+class_labels_list_P+'\\n')\n",
    "        for j in range(0,trees_count):\n",
    "            for k in range(0,id_count):\n",
    "                f.write(str(j)+','+str(k)+',')\n",
    "                for c in range(0,class_count):\n",
    "                        f.write(str(Matrix[j][k][c])+',')\n",
    "                Prefered_Class_Matrix[j][k] = np.argmax(Matrix[j][k])       \n",
    "                f.write(str(np.argmax(Matrix[j][k]))+',')  \n",
    "                for c in range(0,class_count):\n",
    "                    if (np.sum(Matrix[j][k]) != 0):\n",
    "                        f.write(str(round(Matrix[j][k][c]/np.sum(Matrix[j][k]),3))+',')\n",
    "                        Prefered_Class_Matrix_P[j][k][c] = round(Matrix[j][k][c]/np.sum(Matrix[j][k]),3)\n",
    "                    else:\n",
    "                        f.write('0,')\n",
    "                f.write('\\n')    \n",
    "\n",
    "        f.close()            \n",
    "        \n",
    "        \n",
    "        fts_test = dsf.fit_transform(X_test,0)\n",
    "\n",
    "\n",
    "\n",
    "        labels_array = [0] * class_count\n",
    "        labels_array_p = [0] * class_count\n",
    "        predictedClass = 0\n",
    "        correctPredictions = 0\n",
    "        predictedClass_p = 0\n",
    "        correctPredictions_p = 0\n",
    "\n",
    "        for i in range(0,len(fts_test)):\n",
    "            labels_array = [0] * class_count\n",
    "            for j in range(0,trees_count):\n",
    "                for c in range(0,class_count):\n",
    "\n",
    "                    if (Prefered_Class_Matrix[j][fts_test[i][j]] == c):\n",
    "                        labels_array[c] += 1\n",
    "\n",
    "                        labels_array_p[c] += Prefered_Class_Matrix_P[j][fts_test[i][j]][c] \n",
    "\n",
    "            predictedClass =np.argmax(labels_array)\n",
    "            predictedClass_p =np.argmax(labels_array_p)\n",
    "\n",
    "            if (predictedClass == Y_test[i]):\n",
    "                correctPredictions += 1\n",
    "            labels_array = [0] * class_count\n",
    "            if (predictedClass_p == Y_test[i]):\n",
    "                correctPredictions_p += 1\n",
    "            labels_array_p = [0] * class_count\n",
    "\n",
    "        print(correctPredictions)\n",
    "        \n",
    "        print(correctPredictions/len(fts_test))\n",
    "        predictions_results_majority_vote.append(correctPredictions/len(fts_test))\n",
    "        predictions_results_average_probability.append(correctPredictions_p/len(fts_test))\n",
    "        \n",
    "        if (correctPredictions/len(fts_test) > best_prediction_majority_vote):\n",
    "            best_prediction_majority_vote = correctPredictions/len(fts_test)\n",
    "            best_prediction_majority_vote_dsf = 'RF_'+str(rf_depth)+'_t'+str(frequency)\n",
    "            \n",
    "        if (correctPredictions_p/len(fts_test) > best_prediction_average_probability):\n",
    "            best_prediction_average_probability = correctPredictions_p/len(fts_test)\n",
    "            best_prediction_average_probability_dsf = 'RF_'+str(rf_depth)+'_t'+str(frequency)    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f7d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12adf067",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_digits()\n",
    "X = dataset['data']\n",
    "y = dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b2021b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6c10b591c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[4].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6429d4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2615310293571163"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n",
    "scores_ada = cross_val_score(reg_ada, X, y, cv=6)\n",
    "scores_ada.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be7110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
