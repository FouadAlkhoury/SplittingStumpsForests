{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bbf9d3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "FEEL FREE TO IGNORE THIS OUTPUT\n",
      "\n",
      "DSF_NB RF_5_t2_1.json 0.42998872604284105 +- 0.010199037544891675\n",
      "DSF_SVM RF_5_t2_1.json 0.42998872604284105 +- 0.010199037544891675\n",
      "DSF_LR RF_5_t2_1.json 0.42998872604284105 +- 0.010199037544891675\n",
      "Training Time for RF_5_t2_1.json : 0:00:00.595568\n",
      "DSF_NB RF_5_t2_10_3_6.json 0.7767756482525366 +- 0.056211728804139396\n",
      "DSF_SVM RF_5_t2_10_3_6.json 0.8060879368658398 +- 0.044150175076655855\n",
      "DSF_LR RF_5_t2_10_3_6.json 0.8013528748590757 +- 0.04194396936488359\n",
      "Training Time for RF_5_t2_10_3_6.json : 0:00:06.271623\n",
      "Total Training Time: 0:00:06.869131\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import pickle\n",
    "\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import ReadData\n",
    "import cString2json as cString2json\n",
    "import json2graphNoLeafEdgesWithSplitValues as json2graphNoLeafEdgesWithSplitValues\n",
    "from fitModels import fitModels\n",
    "import DecisionSnippetFeatures as DecisionSnippetFeatures\n",
    "import pruning\n",
    "import Forest\n",
    "import datetime\n",
    "from util import writeToReport\n",
    "import numpy as np\n",
    "# %% Parameters. \n",
    "\n",
    "dataPath = \"../data/\"\n",
    "forestsPath = \"../tmp/forests/\"\n",
    "snippetsPath = \"../tmp/snippets/\"\n",
    "resultsPath = \"../tmp/results/\"\n",
    "reportsPath = \"../tmp/reports/\"\n",
    "\n",
    "# current valid options are ['sensorless', 'satlog', 'mnist', 'magic', 'spambase', 'letter', 'bank', 'adult', 'drinking']\n",
    "dataSet = 'satlog'\n",
    "# dataSet = 'adult'\n",
    "# dataSet = 'drinking'\n",
    "\n",
    "# possible forest_types ['RF', 'DT', 'ET']\n",
    "forest_types = ['RF']\n",
    "forest_depths = [5,10,15,20]\n",
    "sigma_values = [0.0,0.1,0.2,0.3]\n",
    "#forest_depths = [5, 10, 15, 20]\n",
    "forest_size = 25\n",
    "\n",
    "maxPatternSize = 6\n",
    "minThreshold = 2\n",
    "maxThreshold = 25\n",
    "\n",
    "scoring_function = 'accuracy'\n",
    "\n",
    "# learners that are to be used on top of Decision Snippet Features\n",
    "learners = {'DSF_NB': MultinomialNB,\n",
    "            'DSF_SVM': LinearSVC, \n",
    "            'DSF_LR': LogisticRegression}\n",
    "\n",
    "# specify parameters that are given at initialization\n",
    "learners_parameters = {'DSF_NB': {},\n",
    "                       'DSF_SVM': {'max_iter': 10000},\n",
    "                       'DSF_LR': {'max_iter': 1000}}\n",
    "\n",
    "\n",
    "run_fit_models = True\n",
    "run_mining = True\n",
    "run_training = True\n",
    "run_eval = True\n",
    "\n",
    "X_train, Y_train = ReadData.readData(dataSet, 'train', dataPath)\n",
    "X_test, Y_test = ReadData.readData(dataSet, 'test', dataPath)\n",
    "X = X_train\n",
    "\n",
    "report_model_dir = reportsPath+'/'+dataSet \n",
    "report_file = report_model_dir + '/report.txt'\n",
    "if not os.path.exists(report_model_dir):\n",
    "    os.makedirs(report_model_dir)\n",
    "    \n",
    "    \n",
    "def dsf_transform(snippets_file, X):\n",
    "    with open(snippets_file, 'r') as f_decision_snippets:\n",
    "\n",
    "        # load decision snippets and create decision snippet features\n",
    "        frequentpatterns = json.load(f_decision_snippets)\n",
    "        dsf = DecisionSnippetFeatures.FrequentSubtreeFeatures(\n",
    "            map(lambda x: x['pattern'], frequentpatterns))\n",
    "        fts = dsf.fit_transform(X)\n",
    "\n",
    "        # transform to onehot encodings for subsequent processing by linear methods\n",
    "        categories = dsf.get_categories()\n",
    "        fts_onehot_sparse = OneHotEncoder(\n",
    "            categories=categories).fit_transform(fts)\n",
    "        fts_onehot = fts_onehot_sparse.toarray()\n",
    "\n",
    "        return fts_onehot\n",
    "\n",
    "start_training_total = datetime.datetime.now()   \n",
    "\n",
    "if run_training:\n",
    "    print('\\n\\nFEEL FREE TO IGNORE THIS OUTPUT\\n')\n",
    "\n",
    "    results_list = list()\n",
    "\n",
    "    # save results list\n",
    "    if not os.path.exists(os.path.join(resultsPath, dataSet)):\n",
    "        os.makedirs(os.path.join(resultsPath, dataSet))\n",
    "\n",
    "    def train_model_on_top(model, fts_onehot, Y_train, scoring_function, model_name, descriptor, scaling=False):\n",
    "\n",
    "        if scaling:\n",
    "            model = Pipeline([('scaler', StandardScaler()), (model_name, model)])\n",
    "\n",
    "        fts_onehot_nb_cv_score = cross_val_score(model, fts_onehot, Y_train, cv=5, scoring=scoring_function)\n",
    "\n",
    "        dsf_score = fts_onehot_nb_cv_score.mean()\n",
    "        dsf_std = fts_onehot_nb_cv_score.std()\n",
    "        print(f'{model_name} {descriptor} {dsf_score} +- {dsf_std}')\n",
    "        writeToReport(report_file, str(model_name) + '\\t' + str(descriptor) + '\\t' + str(dsf_score)  + ' +- ' + str(dsf_std))\n",
    "        model.fit(fts_onehot, Y_train)\n",
    "        return dsf_score, model, fts_onehot_nb_cv_score\n",
    "\n",
    "    # train several models on the various decision snippet features\n",
    "    # store all xval results on traning data in a list\n",
    "    for graph_file in filter(lambda x: x.endswith('.json'), os.listdir(os.path.join(snippetsPath, dataSet))):\n",
    "        \n",
    "        start_training = datetime.datetime.now()\n",
    "        # get Decision Snippet Features\n",
    "        fts_onehot = dsf_transform(os.path.join(snippetsPath, dataSet, graph_file), X_train)\n",
    "       \n",
    "        # train models\n",
    "        for model_type, model_class in learners.items():\n",
    "            xval_score, learner_model, xval_results = train_model_on_top(model_class(**learners_parameters[model_type]), fts_onehot, Y_train, scoring_function, model_type, graph_file)\n",
    "            results_list.append((xval_score, model_type, graph_file, learner_model, xval_results))\n",
    "            # cleanup\n",
    "            xval_score, learner_model, xval_results = None, None, None\n",
    "\n",
    "        # dump after each decision snippet\n",
    "        with open(os.path.join(resultsPath, dataSet, \"training_xval.pkl\"), 'wb') as f_pickle:\n",
    "            pickle.dump(results_list, f_pickle)\n",
    "            \n",
    "            \n",
    "        end_training = datetime.datetime.now()\n",
    "        training_time = (end_training - start_training)\n",
    "        print('Training Time for '+ graph_file +' : '+str(training_time)) \n",
    "        writeToReport(report_file, 'Training Time for '+ graph_file +' : '+str(training_time))    \n",
    "    \n",
    "\n",
    "end_training_total = datetime.datetime.now()\n",
    "training_total_time = (end_training_total - start_training_total)\n",
    "print('Total Training Time: '+str(training_total_time))  \n",
    "writeToReport(report_file, 'Total Training Time: '+str(training_total_time) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cc549b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43805\n"
     ]
    }
   ],
   "source": [
    "x = 5\n",
    "y = pow(5,179)\n",
    "print(y % 56160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1a944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
